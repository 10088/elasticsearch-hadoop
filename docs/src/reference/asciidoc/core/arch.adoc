[[architecture]]
== {eh} and Hadoop

At the core, {eh} integrates two _distributed_ systems: *Hadoop*, a batch-oriented computing platform and *{es}*, a real-time search and analytics engine. From a high-level view both provide a computational component: Hadoop through {mr} and {es} through its search and aggregation.

{eh} goal is to 'connect' these two entities so that they can transparently benefit from each other.

=== {mr} and Shards

A critical component for scalability is parallelism or splitting a task into multiple, smaller ones that execute at the same time, on different nodes in the cluster. The concept is present in both Hadoop through its `splits` (the number of parts in which a source or input can be divided) and {es} through http://www.elasticsearch.org/guide/reference/glossary/#shard[`shards`] (the number of parts in which a index is divided into).

In short, roughly speaking more input splits means more tasks that can read at the same time, different parts of the source. More shards means more 'buckets' from which to read an index content (at the same time).

As such, {eh} uses splits and shards as the main drivers behind the number of tasks executed within the Hadoop and {es} clusters as they have a direct impact on parallelism.

TIP: Hadoop `splits` as well as {es} `shards` play an important role regarding a system behavior - we recommend familiarizing with the two concepts to get a better understanding of your system runtime semantics.

=== Reading from {es}

Shards play a critical role when reading information from {es}. Since it acts as a source, {eh} will create one Hadoop `InputSplit` per {es} shard; that is given a query that works against index `I`, {eh} will dynamically discover the number of shards backing `I` and then for each shard will create an input split (which will determine the number of Hadoop tasks to be executed).
With the default settings, {es} uses *5* http://www.elasticsearch.org/guide/reference/glossary/#primary_shard[`primary`] shards per index which will result in the same number of tasks on the Hadoop side for each query.

NOTE: {eh} does not query the same shards - it iterates through all of them (primaries and replicas) using a round-robin approach. To avoid data duplication, only one shard is used from each shard group (primary and replicas).

A common concern (read optimization) for improving performance is to increase the number of shards and thus increase the number of tasks on the Hadoop side. Unless such gains are demonstrated through benchmarks, we recommend against such a measure since in most cases, an {es} shard can *easily* handle data streaming to a Hadoop task.

=== Writing to {es}

Writing to {es} is driven by the number of input splits (or Hadoop tasks) available. In the current form, {eh} uses only one node on the {es} side to handle the writing which acts as a proxy within the {es} cluster: as data flows in, based on its identity and cluster setting, {es} will route it to the appropriate shard.
The more splits are available, the more reducers can write data in parallel to {es}.

For the vast majority of cases, using only one node on the {es} side should pose no problem - if the network is not saturated, {es} can ingest the data just fine. However, for cases where this happens (again benchmark first instead of guessing), we are working on using multiple nodes based on the available index.

=== Data co-location

Whenever possible, {eh} shares the {es} cluster information with Hadoop to facilitate data co-location. In practice, this means whenever data is read from {es}, the source nodes IPs are passed on to Hadoop to optimize task execution. If co-location is desired/possible, hosting the {es} and Hadoop clusters within the same rack will provide significant network savings.